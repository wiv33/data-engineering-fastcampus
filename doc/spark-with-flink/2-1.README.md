# 2-1 병렬처리와 분산처리

## latency

스파크는 분산 환경에서 병렬 처리로 동작하기 때문에
작성하는 코드가 클러스터에서 어떻게 동작할지 이해하고 코딩을 작성해야 한다.

ex) `reduceByKey`는 각 클러스터 연산을 합치는 동작을 하기 때문에 `클러스터 네트워크 통신`을 한다.
1. RDD.map(A)`.reduceByKey(C)`.filter(B)
2. RDD.map(A).filter(B)`.reduceByKey(C)`


## key-value RDD

함수의 반환 값이 여러 개인 경우 key-value RDD가 된다.

ex)
```python
# python
pairs = rdd.map(lambda x: (x, 1))
```


key value RDD
- 넷플릭스 드라마가 받은 평균 별점(날짜, 승객 수)
- 지역 id 별로 택시 `운행 수`
  - key: 지역 ID
  - value: 운행 수
- 드라마 별로 `별점 수` 모아보기, `평균` 구하기
  - key: 드라마 id
  - value: 별점 수
  - value: 평균
- 이커머스 사이트에서 상품당 `별 평점` 구하기

single value RDD
- 텍스트에 등장하는 단어 수 세기 (날짜)


### reduction function

    `key`가 변경되지 않는 경우 ~~.map()~~ -> `mapValues()` 활용.
    spark 내부적으로 파티션을 유지하여 효율이 증가
    `mapValues()`
    `flatMapValues()`


- reduceByKey()
  - key 값을 기준으로 accumulate 함
- groupByKey()
  - key 값을 기준으로 값을 그룹화
- sortByKey()
  - 키 값을 기준으로 정렬
- keys()
  - 키 추출
- values()
  - 값 추출
- db와 같은 기능
  - join()
  - rightOuterJoin()
  - leftOuterJoin()
  - subtractByKey()



# 실습
1. [restaurant_reviews](../../src/main/scala/_01/CategoryReviewAverage.py)
