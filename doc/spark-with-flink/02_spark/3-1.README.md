# 3-1 Structured VS Unstructured

데이터가 구조화 돼 있다면 spark에서 자동으로 구조를 최적화 한다.

## Unstructured: free form
- 로그 파일
- 이미지

## Semi Structured: 행렬 데이터 (스키마가 있는 데이터)
- CSV
- JSON
- XML

## Structured: 행렬 + 데이터 스키마
- database

## RDD vs Structured Data => SparkSQL

### RDD
스파크가 데이터 구조를 모르기에 데이터를 다루는 개발자가 최적의 구조로 구현 필요.

(최적화가 되어있지 않더라도) 개발자가 구현한 map, flatMap, filter등의 row level 함수를 그대로 수행

### Structured Data

데이터의 구조를 알기  때문에 어떤 테스크를 수행할 것인지 정의만 필요.

스파크에서 최적화도 자동으로 수행


### SparkSQL

유저가 일일이 function을 정의하지 않고 작업 수행 가능.

자동으로 최적화된 구조를 형성한다.


# 3-2 SparkSQL

- 스파크 내부에서 관계형 처리
- 스키마 정보를 이용해 자동 최적화
- 외부 데이터셋 사용 편리함

## Spark 위에 구현된 하나의 패키지

주요 API
- SQL
- DataFrame
- Datasets

주요 Backend component
- Catalyst
  - 쿼리 최적화 엔진
- Tungsten
  - Serializer (직렬화 - 용량 최적화)


### DataFrame

각 코어에서 다루는 데이터형

- Spark Core - RDD
- Spark SQL - DataFrame

`DataFrame`은 테이블 데이터 셋이고, 
`RDD`에 `scheme`이 적용된 형태

### SparkSession

- Spark Core - `SparkContext`
- Spark SQL - `SparkSession`

ex) 
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test-app").getOrCreate()
```

## DataFrame 만드는 방법

### RDD
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test-app").getOrCreate()
spark.createDataFrame(data, scheme)
```
- data: required
- scheme: optional
    - 스키마 내 정의된 타입이 `없는` 경우: 스키마를 자동 유추
    - 스키마 내 정의된 타입이 `있는` 경우: 스키마를 사용자가 정의

### FILE
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("test-app").getOrCreate()
df_json = spark.read.json("dataset/nyt2.json")
df_csv = spark.read.csv("csv_data.csv")
df_parquet = spark.read.load("parquet_data.parquet")
```


### DataFrame to table
`DataFrame`을 데이터베이스 테이블로 사용

`SparkSession`이 필요

```python
data.createOrReplaceTempView("mobility_data")  # 1
spark.sql("SELECT pickup_datatime FROM mobility_data LIMIT 5").show()  # 2
```
1. `mobility_data` 닉네임 설정 
2. 쿼리 작성


## DataSet

타입이 있는 `DataFrame`이다.


# 3-3. SQL 기초

[실습](../../../src/main/scala/_02/learn_sql.ipynb)

# 3-4. DataFrame

관계형 데이터 셋으로 RDD + Relation 형태이다.

`RDD`는 함수형 API 지원.

`DataFrame`은 선언형 API 지원.

최적화 기능 자동화

타입이 없다.

## DataFrame 확장 기능

RDD와 같은 방식

- 지연 실행 (lazy execution)
- 분산 저장
- Immutable

확장된 API
- 열 (row) 객체 지원
- SQL 쿼리 실행 지원
- 스키마 지원과 이를 바탕으로 성능 최적화 지원
- CSV, JSON, Hive 등으로 읽어오거나 변환 지원

### df 스키마 확인 방법

- spark.`dtypes`
- spark.sql.`show()`
- `printSchema()`
  - 스키마를 트리(tree) 형태로 출력한다.
- ArrayType
- MapType
- StructType
  - Object 형태

### SQL  작업

- SELECT
  - ``df.select('*').collect()`` lazy 동작으로 collect() 종단 연산을 실행해야 함.
  - `df.select('name', 'age')).collect()`
  - `df.select(df.name, (df.age + 10).alias('age'))`
- agg - Aggregate
  - `df.agg({"age": "max"}).collect()`
    - [Row(max(age)=5)] 반환
  - `df.agg(F.min(df.age)).collect()` `from pyspark.sql import functions as F` 활용
    - pyspark.sql function 함수 사용
    - [Row(min(age)=2)] 반환
- groupBy
  - `df.groupby().avg().collect()`
  - `df.groupBy('name').agg({'age': 'mean'}).collect()`
  - `df.groupBy(df.name).avg().collect()`
- join
  - `df.join(df2, 'name').select(df.name, df2.height).collect()`
    - [Row(name='Bob', height=85)]


# 3-5. SQL로 트립 수 세기

[실습: 트립 수 세기](../../../src/main/scala/_02/trip_count_sql.ipynb)

# 3-6. SQL로 뉴욕의 각 행정구 데이터 추출

[실습: zone 기준으로 트립 수 세기](../../../src/main/scala/_02/trip_count_sql_by_zone_sql.ipynb)

# 3-7. catalyst optimizer 및 tungsten project 작동 원리
쿼리를 돌리기 이ㅜ해 실행하는 엔진
- Catalyst
  - SQL, DataFrame 에서 작성한 query 받고
  - Query plan, optimization 을 수행
- Tungsten 
  - low 레벨에서 메모리와 CPU 효율을 높이는 작업을 수행

interface `Catalyst`가 있기 때문에 `DataFrame`, `SQL`을 수행할 수 있다

## Logical plan

수행할 모든 transformation 단계에 대한 추상화

데이터가 어떻게 변해야 하는지 정의만 하며,

실제 어디서 어떻게 동작하는지 정의하지 않음


## Physical plan

logical plan이 어떻게 클러스터 위에서 실행될지 정의 

실행 전략을 만들고 cost model에 따라 최적화 수행



## logical plan to physical plan

1. 분석
   1. DataFrame 객체의 relation을 계산하고 컬럼 타입과 이름 확인
2. Logical plan 최적화
   1. 상수로 표현된 식을 compile time에 계산
   2. predicate pushdown: join & filter -> filter & join
   3. projection pruning: 연산에 필요한 컬럼만 추출 (join 결과의 컬럼이 3개일 때 하위 테이블의 수백 개 중 3개만 load)
3. physical plan 만들기: spark에서 실행 가능한 plan으로 변환
4. 코드 제너레이션: 최적화된 physical plan을 java bytecode로 변황


### explain(true)

    query 실행 게획을 확인할 수 있다.

예시)
```python
spark.sql("select borough, count(*) as tiprs "
          "from (select zone_data.Borough as borough"
                " from trip_data join zone_data on trip_data.DOLocationID = zone_data.LocationID) group by borough order by tiprs desc").explain(True)
```
- parsed logical plan
- analyzed logical plan
- optimized logical plan
- physical plan

## Tungsten project

physical plan이 선택된 후 분산 환경에서 실행될 Bytecode가 만들어지게 되는데, 

이 프로세스를 `Code Generation`이라고 한다.

스파크 엔진의 성능 향상이 목적
- 메모리 관리 최적화
- 캐시 활용 연산
- 코드 생성


# 3-8. User Defined functions

[실습: ](../../../src/main/scala/_02/user_defiled_functions.ipynb)

# 3-9. 뉴욕 택시 데이터 분석

[실습](../../../src/main/scala/_02/taxi_analysis.ipynb)