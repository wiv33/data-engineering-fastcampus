# 3-1. explain

## 실행 계획 확인하기

```python
from pyflink.table import EnvironmentSettings, TableEnvironment

env_settings = EnvironmentSettings.in_streaming_mode()
t_env = TableEnvironment.create(environment_settings=env_settings)

# hard coding
col_names = ["id", "language"]
d = [
    (1, "php"),
    (2, "python"),
    (3, "c++"),
    (4, "java"),
]
t1 = t_env.from_elements(d, col_names)
t2 = t_env.from_elements(d, col_names)

# p로 시작하는 value
table = t1.where(t1.language.like("p%")).union_all(t2)
print(table.explain())
```

# 3-2. 여러 개의 Sink를 동시에 만들기

```python
from pyflink.table import EnvironmentSettings, TableEnvironment

env_settings = EnvironmentSettings.in_streaming_mode()
t_env = TableEnvironment.create(env_settings)

col_names = ["id", "lang"]
data = [
    (1, "php"),
    (2, "python"),
    (3, "c++"),
    (4, "java")
]

t1 = t_env.from_elements(data, col_names)
t2 = t_env.from_elements(data, col_names)
t_env.execute_sql("""
  CREATE TABLE print_sink1 (
    id BIGINT,
    lang VARCHAR
  ) WITH (
    'connector' = 'print'
  )
""")
t_env.execute_sql("""
  CREATE TABLE print_sink2 (
    id BIGINT,
    lang VARCHAR
  ) WITH (
    'connector' = 'print'
  )
""")

statement_set = t_env.create_statement_set()
statement_set.add_insert("print_sink1", t1.where(t1.lang.like("p%")))
statement_set.add_insert("print_sink2", t2)

statement_set.execute().wait()
# print(statement_set.explain())
```

# 3-3. DataStream과 Table 같이 사용

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment
from pyflink.common.typeinfo import Types

env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(['c', 'python', 'php', 'java'], Types.STRING())
t = t_env.from_data_stream(ds)

t_env.create_temporary_view('lang', t)
res_table = t_env.sql_query("SELECT * FROM lang WHERE f0 like 'p%'")

res_ds = t_env.to_data_stream(res_table)
res_ds.print()
env.execute()
```

# 3-4. 끝이 있는 테이블과 다이나믹 테이블

## batch table

- source 테이블 행의 수가 정해져 있다.
- Flink 앱이 시작될 때 모든 데이터에 접근이 가능
- Shuffle 가능
    - 모든 데이터 필터
    - 데이터 조인 가능
- 데이터 처리 후 실행이 멈추게 된다.

## Streaming table

- Source의 행의 수가 정해져 있지 않고 계속 변한다.
- 데이터는 과거부터 지금까지의 데이터만 접근 가능
- 새로운 데이터가 계속 생긴다.

Data -> Dynamic Source Table -> Flink Operation (**State**) -> Dynamic Sink Table -> Data

- Dynamic Source table이 계속 가변
- State
    - 일/월단위 데이터 모으기
- Sink Table
    - file
    - kafka 등으로 보냄.

# 3-5. Table Operation

## Projection

- select
- filter
    - where
    - filter
- aggregation
    - group By
- calculation
    - avg
    - sum
    - count

## 연산 후 새로운 테이블을 반환 (Immutable)

# 3-6. Select

```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment
)

t_env = TableEnvironment.create(
    EnvironmentSettings.in_streaming_mode())
t_env.get_config().get_configuration().set_string("parallelism.default", "1")

input_path = "tripsdata/sample_trips.csv"
source_ddl = f"""
  create table sample_trips (
    VendorID INT,
    tpep_pickup_datetime STRING,
    tpep_dropoff_datetime STRING,
    passenger_count INT,
    trip_distance DOUBLE,
    RatecodeID INT,
    store_and_fwd_flag STRING,
    PULocationID INT,
    DOLocationID INT,
    payment_type INT,
    fare_amount DOUBLE,
    extra DOUBLE,
    mta_tax DOUBLE,
    tip_amount DOUBLE,
    tolls_amount DOUBLE,
    improvement_surcharge DOUBLE,
    total_amount DOUBLE,
    congestion_surcharge DOUBLE
  ) with (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path' = '{input_path}',
    'csv.ignore-parse-errors' = 'true'
  )
"""

t_env.execute_sql(source_ddl)
tbl = t_env.from_path("sample_trips")

# 기본적인 select
print("===========BASIC SELECT============")
r1 = tbl.select(
    tbl.PULocationID.alias("pickup_location_id"),
    tbl.total_amount
)
print(r1.to_pandas())

r1_sql = t_env.sql_query("""
  SELECT 
    PULocationID AS pickup_location_id, 
    total_amount
  FROM sample_trips
""")
print(r1.to_pandas())

# distinct
print("===========DISTINCT============")
distinct_pu_loc = tbl.select(
    tbl.PULocationID.alias("pickup_location_id")
).distinct()
print(distinct_pu_loc.to_pandas())

distinct_pu_loc_sql = t_env.sql_query("""
  SELECT DISTINCT PULocationID AS pickup_location_id FROM sample_trips
""")
print(distinct_pu_loc_sql.to_pandas())

# 간단한 계산
# 한명당 택시비
print("===========CALCULATION============")
ppp = tbl.select(tbl.total_amount / tbl.passenger_count)
print(ppp.to_pandas())

ppp_sql = t_env.sql_query("""
  SELECT total_amount / passenger_count AS price_per_person
  FROM sample_trips
""")
print(ppp_sql.to_pandas())

```

# 3-7. Filter

```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment
)
from pyflink.table.expressions import col  # column 선택할 때 사용

t_env = TableEnvironment.create(
    EnvironmentSettings.in_streaming_mode())
t_env.get_config().get_configuration().set_string("parallelism.default", "1")

input_path = "tripsdata/sample_trips.csv"
source_ddl = f"""
  create table sample_trips (
    VendorID INT,
    tpep_pickup_datetime STRING,
    tpep_dropoff_datetime STRING,
    passenger_count INT,
    trip_distance DOUBLE,
    RatecodeID INT,
    store_and_fwd_flag STRING,
    PULocationID INT,
    DOLocationID INT,
    payment_type INT,
    fare_amount DOUBLE,
    extra DOUBLE,
    mta_tax DOUBLE,
    tip_amount DOUBLE,
    tolls_amount DOUBLE,
    improvement_surcharge DOUBLE,
    total_amount DOUBLE,
    congestion_surcharge DOUBLE
  ) with (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path' = '{input_path}',
    'csv.ignore-parse-errors' = 'true'
  )
"""

t_env.execute_sql(source_ddl)
tbl = t_env.from_path("sample_trips")

# print("===============WHERE=================")
r1 = tbl.select(
    tbl.total_amount
).where(col('total_amount') >= 10)
print(r1.to_pandas())

r1_sql = t_env.sql_query("""
  SELECT total_amount FROM sample_trips WHERE total_amount >= 10
""")
print(r1_sql.to_pandas())

print("===============WHERE ON CALCUATION=================")
r2 = tbl.select(
    (tbl.total_amount / tbl.passenger_count).alias("ppp")
).where(col("ppp") >= 10)
print(r2.to_pandas())

r2_sql = t_env.sql_query("""
  SELECT * FROM (
    SELECT total_amount / passenger_count AS ppp
    FROM sample_trips
  ) WHERE ppp >= 10 
""")
print(r2_sql.to_pandas())

```
# 3-8. Join
```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment
)
from pyflink.table.expressions import col

t_env = TableEnvironment.create(
    EnvironmentSettings.in_streaming_mode())
t_env.get_config().get_configuration().set_string("parallelism.default", "1")

trips_path = "tripsdata/sample_trips.csv"
sample_trips_ddl = f"""
  create table sample_trips (
    VendorID INT,
    tpep_pickup_datetime STRING,
    tpep_dropoff_datetime STRING,
    passenger_count INT,
    trip_distance DOUBLE,
    RatecodeID INT,
    store_and_fwd_flag STRING,
    PULocationID INT,
    DOLocationID INT,
    payment_type INT,
    fare_amount DOUBLE,
    extra DOUBLE,
    mta_tax DOUBLE,
    tip_amount DOUBLE,
    tolls_amount DOUBLE,
    improvement_surcharge DOUBLE,
    total_amount DOUBLE,
    congestion_surcharge DOUBLE
  ) with (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path' = '{trips_path}',
    'csv.ignore-parse-errors' = 'true'
  )
"""

zone_path = "./tripsdata/taxi+_zone_lookup.csv"
zone_ddl = f"""
  create table zones (
    LocationID INT,
    Borough STRING,
    Zone STRING,
    service_zone STRING
  ) with (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path' = '{zone_path}',
    'csv.ignore-parse-errors' = 'true'
  )
"""

t_env.execute_sql(sample_trips_ddl)
t_env.execute_sql(zone_ddl)
trips = t_env.from_path("sample_trips")
zones = t_env.from_path("zones")


# join
r1 = trips.join(zones, trips.PULocationID == zones.LocationID).select(zones.Zone, trips.total_amount)

print(r1.to_pandas())

r1_sql = t_env.sql_query("""
SELECT zones.Zone, sample_trips.total_amount
FROM sample_trips JOIN zones ON sample_trips.PULocationID = zones.LocationID
""")

print(r1_sql.to_pandas())

# left outer join
r2 = trips.left_outer_join(zones, trips.PULocationID == zones.LocationID).select(zones.Zone, trips.total_amount)

print(r2.to_pandas())

r2_sql = t_env.sql_query("""
SELECT zones.Zone, sample_trips.total_amount
FROM sample_trips LEFT OUTER JOIN zones ON sample_trips.PULocationID = zones.LocationID
""")

print(r2_sql.to_pandas())

```
# 3-9. GroupBy

```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment
)

t_env = TableEnvironment.create(
    EnvironmentSettings.in_streaming_mode())
t_env.get_config().get_configuration().set_string("parallelism.default", "1")

trips_path = "tripsdata/sample_trips.csv"
sample_trips_ddl = f"""
  create table sample_trips (
    VendorID INT,
    tpep_pickup_datetime STRING,
    tpep_dropoff_datetime STRING,
    passenger_count INT,
    trip_distance DOUBLE,
    RatecodeID INT,
    store_and_fwd_flag STRING,
    PULocationID INT,
    DOLocationID INT,
    payment_type INT,
    fare_amount DOUBLE,
    extra DOUBLE,
    mta_tax DOUBLE,
    tip_amount DOUBLE,
    tolls_amount DOUBLE,
    improvement_surcharge DOUBLE,
    total_amount DOUBLE,
    congestion_surcharge DOUBLE
  ) with (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path' = '{trips_path}',
    'csv.ignore-parse-errors' = 'true'
  )
"""

t_env.execute_sql(sample_trips_ddl)
trips = t_env.from_path("sample_trips")

# group by
r1 = trips.select(trips.PULocationID, trips.total_amount).group_by(trips.PULocationID).select(trips.PULocationID,
                                                                                              trips.total_amount.sum.alias(
                                                                                                  'total'))
print(r1.to_pandas())

r1_sql = t_env.sql_query("""
SELECT PULocationID, SUM(total_amount ) as total 
FROM sample_trips GROUP BY PULocationID
""")

print(r1_sql.to_pandas())

```