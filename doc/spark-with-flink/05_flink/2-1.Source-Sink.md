# 2-1. Table API

## Data Structure
- Table Api와 SQL 인터페이스 모두 Flink의 공통 Table 데이터 스트럭처 사용
- Source에 쿼리할 수 있고, table object를 반환한다.
- Table Object엔 관계형 API나 DataStream API를 연계해서 사용 가능하다.
- Lazy Evaluation
- 이름은 `카탈로그`.`데이터베이스`.`테이블 이름`
- 실행 환경에서 정의되며 다른 실행 환경으로부터 `접근이 불가`
## table 종류

Temporary Tables
- 플링크 세션 동안만 존재하는 테이블

Permanent Tables
- 외부 메타데이터 카탈로그에 의해 참조될 수 있는 테이블
  - 여러 flink 세션에서 사용 가능


## table environment
- table api나 sql 모두 공통적으로 table environment를 통해 flink 런터암에 접근한다. (Batch / Streaming)
- flink와 상호작용할 수 있게 만들어 준다.
  - 데이터베이스, 테이블, 뷰, UDF, 카탈로그 등을 등록
  - 테이블 생성 / 등록 (Temporary, Permanent)
  - UDF 등록
  - SQL 쿼리 실행
  - 모듈 / 라이브러리 실행
  - Table API와 DataStream API 연동
- 쿼리 플래너: Legacy flink & Blink (새로운 버전)


# 2-2. Table Environment 만들기
```python
from pyflink.table import EnvironmentSettings, TableEnvironment

# batch 환경
batch_settings = EnvironmentSettings.new_instance() \
    .in_batch_mode() \
    .build()

batch_table_env = TableEnvironment.create(batch_settings)

# Stream 환경
stream_settings = EnvironmentSettings.new_instance().in_streaming_mode().build()

stream_table_env = TableEnvironment.create(stream_settings)
```

```python
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

datastream_env = StreamExecutionEnvironment.get_execution_environment()
table_env = StreamTableEnvironment.create(datastream_env)

```

# 2-3 ~ 2-4. Source Table 만들기
```python
from pyflink.table import EnvironmentSettings, TableEnvironment, DataTypes

settings = EnvironmentSettings.new_instance().in_batch_mode().build()
table_env = TableEnvironment.create(settings)

sample_data = [
  ("spark", 1),
  ("Airflow", 2),
  ("Kafka", 3),
  ("Flink", 4)
]

src1 = table_env.from_elements(sample_data)
print(src1)
src1.print_schema()

df = src1.to_pandas()
print(df)

column = ["framework", "chapter"]
src2 = table_env.from_elements(sample_data, column)

print(src2.to_pandas())

schema = DataTypes.ROW([
  DataTypes.FIELD("framework", DataTypes.STRING()),
  DataTypes.FIELD("chapter", DataTypes.BIGINT()),
])

src3 = table_env.from_elements(sample_data, schema)
print(src3.to_pandas())
```

# 2-5. CSV Source Table 만들기
```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment, DataTypes,
    CsvTableSource
)

settings = EnvironmentSettings.new_instance() \
    .in_batch_mode().build()
table_env = TableEnvironment.create(settings)

field_names = ["framework", "chapter"]
field_types = [DataTypes.STRING(), DataTypes.BIGINT()]

source = CsvTableSource(
    "./sample.csv",
    field_names,
    field_types,
    ignore_first_line=False
)

table_env.register_table_source("chapters", source)
table = table_env.from_path("chapters")

print(table.to_pandas())
```


# 2-6. Stream 환경에서 CSV Source Table 만들기

## function
```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment,
    Schema, DataTypes, TableDescriptor
)

t_env = TableEnvironment.create(
    EnvironmentSettings.in_streaming_mode())
t_env.get_config().get_configuration().set_string("parallelism.default", "1")

input_path = "./sample.csv"

t_env.create_temporary_table(
    "source",
    TableDescriptor.for_connector("filesystem")
    .schema(Schema.new_builder()
            .column("framework", DataTypes.STRING())
            .column("chapter", DataTypes.INT())
            .build())
    .option("path", input_path)
    .format("csv")
    .build())

src = t_env.from_path("source")
print(src.to_pandas())
```

## SQL
```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment
)

t_env = TableEnvironment.create(
    EnvironmentSettings.in_streaming_mode())
t_env.get_config().get_configuration().set_string("parallelism.default", "1")

input_path = "./sample.csv"
source_ddl = f"""
  create table source (
    framework STRING,
    chapter INT
  ) with (
    'connector' = 'filesystem',
    'format' = 'csv',
    'path' = '{input_path}'
  )
"""

t_env.execute_sql(source_ddl)
src = t_env.from_path("source")

print(src.to_pandas())
```


# 2-7. Kafka Source Table 만들기

```python
import os
from pyflink.common.serialization import SimpleStringSchema
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer
from pyflink.datastream.execution_mode import RuntimeExecutionMode
from pyflink.table import StreamTableEnvironment

env = StreamExecutionEnvironment.get_execution_environment()
env.set_runtime_mode(execution_mode=RuntimeExecutionMode.STREAMING)
env.enable_checkpointing(1000)
env.get_checkpoint_config().set_max_concurrent_checkpoints(1)
t_env = StreamTableEnvironment.create(env)

kafka_jar_path = os.path.join(
  os.path.abspath(os.path.dirname(__file__)), "../",
  "flink-sql-connector-kafka_2.11-1.14.0.jar"
)
t_env.get_config().get_configuration().set_string(
  "pipeline.jars", f"file://{kafka_jar_path}"
)


schema = SimpleStringSchema()
kafka_consumer = FlinkKafkaConsumer(
  topics="flink-test",
  deserialization_schema=schema,
  properties={
    'bootstrap.servers': 'localhost:9092',
    'group.id': 'test_group'
  })

ds = env.add_source(kafka_consumer)
t_env.execute_sql("""
  CREATE TABLE blackhole (
    data STRING
  ) WITH (
    'connector' = 'blackhole'
  )
""")

table = t_env.from_data_stream(ds)
table.insert_into("blackhole")
t_env.execute("flink_kafka_consumer")
```

# 2-8. SQL로 카프카와 연결하기
```python
import os 
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)

kafka_jar_path = os.path.join(
  os.path.abspath(os.path.dirname(__file__)), "../",
  "flink-sql-connector-kafka_2.11-1.14.0.jar"
)
t_env.get_config().get_configuration().set_string(
  "pipeline.jars", f"file://{kafka_jar_path}"
)

souce_query = f"""
  create table source (
    framework STRING,
    chapter INT
  ) with (
    'connector' = 'kafka',
    'topic' = 'flink-test',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'test-group',
    'format' = 'csv',
    'scan.startup.mode' = 'earliest-offset'
  )
"""

t_env.execute_sql(souce_query)

sink_query = """
  CREATE TABLE blackhole (
    framework STRING,
    chapter INT
  ) WITH (
    'connector' = 'blackhole'
  )
"""

t_env.execute_sql(sink_query)
t_env.from_path("source").insert_into("blackhole")
t_env.execute("flink_kafka_consumer_sql")
```

# 2-9. Sink

sink 테이블 만드는 방법.
- file system
  - csv
  - ...
- DB
  - RDB
  - time series db
- message Queue
  - kafka
  - rabbitMQ

# 2-10. CSV Sink Table 만들기

저장하는 방법
```python
from pyflink.table import (
    EnvironmentSettings, TableEnvironment, DataTypes,
    CsvTableSource, CsvTableSink, WriteMode
)

settings = EnvironmentSettings.new_instance() \
    .in_batch_mode().use_blink_planner().build()
t_env = TableEnvironment.create(settings)

in_field_names = ["framework", "chapter"]
in_field_types = [DataTypes.STRING(), DataTypes.BIGINT()]

source = CsvTableSource(
    "./sample.csv",
    in_field_names,
    in_field_types,
    ignore_first_line=False
)

t_env.register_table_source("chapters", source)
table = t_env.from_path("chapters")

print("--- print schema ---")
table.print_schema()

out_field_names = ["framework", "chapter"]
out_field_types = [DataTypes.STRING(), DataTypes.BIGINT()]
sink = CsvTableSink(
    out_field_names,
    out_field_types,
    './sample_copy.csv',
    num_files=1,
    write_mode=WriteMode.OVERWRITE
)
t_env.register_table_sink('out', sink)

table.insert_into('out') 
t_env.execute("sample_copy")
```

# 2-11. kafka Sink DataStream
ingest kafka  -> flink  -> egress kafka

## ingest topic

example_source


## egress topic

example_destination

```python
import os
from pyflink.common.serialization import SimpleStringSchema
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.connectors import FlinkKafkaConsumer, FlinkKafkaProducer
from pyflink.datastream.execution_mode import RuntimeExecutionMode 

env = StreamExecutionEnvironment.get_execution_environment()
env.set_runtime_mode(execution_mode=RuntimeExecutionMode.STREAMING)
env.enable_checkpointing(1000)
env.get_checkpoint_config().set_max_concurrent_checkpoints(1)

kafka_jar_path = os.path.join(
  os.path.abspath(os.path.dirname(__file__)), "../",
  "flink-sql-connector-kafka_2.11-1.14.0.jar"
)
env.add_jars(f"file://{kafka_jar_path}")

# 인입되는 토픽
in_schema = SimpleStringSchema()
kafka_consumer = FlinkKafkaConsumer(
  topics="example-source",
  deserialization_schema=in_schema,
  properties={
    "bootstrap.servers": "localhost:9092",
    "group.id": "test_group"
  })

# 나가는 토픽
out_schema = SimpleStringSchema()
kafka_producer = FlinkKafkaProducer(
  topic="example-destination",
  serialization_schema=out_schema,
  producer_config={
    "bootstrap.servers": "localhost:9092"
  })

ds = env.add_source(kafka_consumer)
ds.add_sink(kafka_producer)

env.execute("kafka_to_kafa")
```

# 2-12. kafka Sink Table 만들기

```python
import os
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment, EnvironmentSettings

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
settings = EnvironmentSettings.new_instance().in_streaming_mode().use_blink_planner().build()
t_env = StreamTableEnvironment.create(env, environment_settings=settings)

kafka_jar_path = os.path.join(
  os.path.abspath(os.path.dirname(__file__)), "../",
  "flink-sql-connector-kafka_2.11-1.14.0.jar"
)
t_env.get_config().get_configuration().set_string(
  "pipeline.jars", f"file://{kafka_jar_path}"
)

# 인입되는 토픽
source_query = f"""
  create table source (
    framework STRING,
    chapter INT
  ) with (
    'connector' = 'kafka',
    'topic' = 'example-source',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.group.id' = 'test-group',
    'format' = 'json',
    'scan.startup.mode' = 'latest-offset'
  )
"""

# 나가는 토픽
sink_query = """
  CREATE TABLE sink (
    framework STRING,
    chapter INT
  ) WITH (
    'connector' = 'kafka',
    'topic' = 'example-destination',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json',
    'scan.startup.mode' = 'latest-offset'
  )
"""

t_env.execute_sql(source_query)
t_env.execute_sql(sink_query)
t_env.from_path("source").insert_into("sink")
t_env.execute("flink_kafka_to_kafka_sql")
```