# 4-1. Windowing

- 스트림 프로세싱의 꽃
- 스트림으로 들어오는 데이터를 묶어서 처리하는 기능
  - Sensor 데이터에서 노이즈를 줄일 때
  - 세션을 처리할 때
- Aggregation
  - 배치 프로세싱과는 다르게 동작한다.
- 가장 최신의 데이터를 업데이트 하는 개념

## Key

- `windows operation`은 `Keyed`, `non-keyed` 두 가지 방식 지원
- key로 파티션 설정 가능
  - 같은 키끼리 묶어서 처리
  - 다른 키와 분리
- key를 기준으로 병렬 처리가 가능

keyed
stream().key_by().window()

non-keyed
stream().window()

## Windows 세 가지 종류
- Tumbling
- Sliding
- Session

## Functions

Window assigner를 정의한 후 window 안에서 일어날 계산을 정의하는 함수들

### ProcessWindowFunction
- window 안에 존재하는 모든 요소를 반환
- 시간과 state 정보를 포함하는 context도 같이 반환
- `ProcessWindowFunction class`를 사용자가 구현해서 사용한다.
  - def process
  - def clear

### WindowFunction - Deprecated
- Process WindowFunction의 오래된 버전
- context 정보를 많이 갖고 있지 않음


# 4-2. Tumbling VS Sliding VS Session Windows

## Tumbling
- 시간 블록마다 데이터를 모아볼 때
- 지정한 시간만큼 window를 열고 닫아서 묶어서 처리
- ex) 직전 1분 통계를 내고 그 전 1분 단위씩 통계를 작성할 때
- `중복되는 데이터가 없이 처리` 가능

## Sliding
- 시간 블록마다 데이터를 모아볼 때
  - 업데이트 주기가 시간 블록보다 짧은 작업
- 최신 1분 단위 데이터를 가져오는데 10초마다 업데이트
- 예) 주식
- `중복된 데이터 존재`

## Session
- window에 새로운 이벤트가 들어오는 기준으로 window의 생성과 끝이 결정
- ex) 한번 시작하면 얼마나 많이 하는지 분석할 때
  - 유저 행동 분석
- session gap 설정이 필요


# 4-3. Event Time, Watermark
- 데이터의 발생 시간과 시스템에서 처리되는 시간에 차이가 있다.
- 데이터가 늦게 들어올 수 있다.
- 데이터를 모아보는 window 작업할 때 문제가 될 수 있다.
- Processing Time을 기준으로 하면 쉽지만 `정확한 통계를 얻기 힘들다.`
- Event time을 기준으로 처리를 하면 `나중에 들어올 데이터까지 고려`해야 한다.

## Event time
- time column을 추가해서 `이벤트가 발생한 시간`을 `데이터 안에 삽입`.
- 메시지 도착까지 지연이 발생한 경우 `처리 시간 기반 시스템`과 결과가 달라진다.
- 윈도우가 닫히고 난 뒤, 유입된 메시지는 처리 불가
- `지각하는 메시지 처리 기준 설정` 필요 - watermark

## watermark

- 지연된 메시지를 처리하기 위한 아이디어
- watermark는 하나의 `Timestamp`
- watermark보다 지연된 메시지는 `도착하지 않을 것`이라고 가정
- watermark 설정 예) 현재 시간 - 5초
  - 메시지의 지연 시간을 5초까지는 허용하겠다고 알려주는 것
- 윈도우 1초 - 10초의 결과 값은 `15초`에 나오게 된다.

ex)
```python
"""
CREATE TABLE sample (
   event_time TIMESTAMP(3)
   WATERMARK FOR event_time as event_time - INTERVAL '5' SECOND 
) WITH (
    'connector' = 'kafka',
    'topic' = 'test-topic',
    'properties.bootstrap.servers' = 'localhost:9092',
    'format' = 'json'
)
"""
```


# 4-4 ~ 4-5. Tumbling Windows 구현

## 4-4. Table API 사용
```python
from pyflink.common.time import Instant
from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import (
    DataTypes, TableDescriptor, Schema, StreamTableEnvironment
)
from pyflink.table.expressions import lit, col
from pyflink.table.window import Tumble

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(
    collection=[
        (Instant.of_epoch_milli(1000), "Alice", 110.1),
        (Instant.of_epoch_milli(4000), "Bob", 30.2),
        (Instant.of_epoch_milli(3000), "Alice", 20.0),
        (Instant.of_epoch_milli(2000), "Bob", 53.1),
        (Instant.of_epoch_milli(5000), "Alice", 13.1),
        (Instant.of_epoch_milli(3000), "Bob", 3.1),
        (Instant.of_epoch_milli(7000), "Bob", 16.1),
        (Instant.of_epoch_milli(10000), "Alice", 20.1),
    ],
    type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))
"""
# timestamp 생성, f0 첫 번째 컬럼, TIMESTAMP(3 == precision 얼마나 정밀한지)
...
.column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
...
"""
table = t_env.from_data_stream(
    ds,
    Schema.new_builder()
    .column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
    .column("f1", DataTypes.STRING())
    .column("f2", DataTypes.FLOAT())
    .watermark("ts", "ts - INTERVAL '3' SECOND")
    .build()
).alias("ts", "name", "price")

# table 등록
t_env.create_temporary_table(
    "sink",
    TableDescriptor.for_connector("print")
    .schema(Schema.new_builder()
            .column("name", DataTypes.STRING())
            .column("total_price", DataTypes.FLOAT())
            .column("w_start", DataTypes.TIMESTAMP(3))
            .column("w_end", DataTypes.TIMESTAMP(3))
            .build()
            ).build())

t = table.window(Tumble.over(lit(5).seconds).on(col("ts")).alias("w")) \
    .group_by(table.name, col("w")) \
    .select(table.name, table.price.sum, col("w").start, col("w").end)

t.execute_insert("sink").wait()
```


## 4-5. SQL 사용

```python
from pyflink.common.time import Instant
from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import (
    DataTypes, TableDescriptor, Schema, StreamTableEnvironment
)
from pyflink.table.expressions import lit, col
from pyflink.table.window import Tumble

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(
    collection=[
        (Instant.of_epoch_milli(1000), "Alice", 110.1),
        (Instant.of_epoch_milli(4000), "Bob", 30.2),
        (Instant.of_epoch_milli(3000), "Alice", 20.0),
        (Instant.of_epoch_milli(2000), "Bob", 53.1),
        (Instant.of_epoch_milli(5000), "Alice", 13.1),
        (Instant.of_epoch_milli(3000), "Bob", 3.1),
        (Instant.of_epoch_milli(7000), "Bob", 16.1),
        (Instant.of_epoch_milli(10000), "Alice", 20.1),
    ],
    type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))
"""
# timestamp 생성, f0 첫 번째 컬럼, TIMESTAMP(3 == precision 얼마나 정밀한지)
...
.column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
...
"""
table = t_env.from_data_stream(
    ds,
    Schema.new_builder()
    .column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
    .column("f1", DataTypes.STRING())
    .column("f2", DataTypes.FLOAT())
    .watermark("ts", "ts - INTERVAL '3' SECOND")
    .build()
).alias("ts", "name", "price")
# window table 생성
t_env.create_temporary_view('source_table', table)
windowed = t_env.sql_query("""
SELECT name, SUM(price) AS total_price,
        TUMBLE_START(ts, INTERVAL '5' SECONDS) as w_start,
        TUMBLE_END(ts, INTERVAL '5' SECONDS) as w_end
FROM source_table
GROUP BY TUMBLE(ts, INTERVAL '5' SECONDS),
name
""")

# table 등록
t_env.create_temporary_table(
    "sink",
    TableDescriptor.for_connector("print")
    .schema(Schema.new_builder()
            .column("name", DataTypes.STRING())
            .column("total_price", DataTypes.FLOAT())
            .column("w_start", DataTypes.TIMESTAMP(3))
            .column("w_end", DataTypes.TIMESTAMP(3))
            .build()
            ).build())


windowed.execute_insert("sink").wait()
```

# 4-6 ~ 4-7. Sliding Windows 구현

## 4-6. Table Api 사용

```python
from pyflink.common.time import Instant
from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import (
    DataTypes, TableDescriptor, Schema, StreamTableEnvironment
)
from pyflink.table.expressions import lit, col
from pyflink.table.window import Slide

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(
    collection=[
        (Instant.of_epoch_milli(1000), "Alice", 110.1),
        (Instant.of_epoch_milli(4000), "Bob", 30.2),
        (Instant.of_epoch_milli(3000), "Alice", 20.0),
        (Instant.of_epoch_milli(2000), "Bob", 53.1),
        (Instant.of_epoch_milli(5000), "Alice", 13.1),
        (Instant.of_epoch_milli(3000), "Bob", 3.1),
        (Instant.of_epoch_milli(7000), "Bob", 16.1),
        (Instant.of_epoch_milli(10000), "Alice", 20.1),
    ],
    type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))
"""
# timestamp 생성, f0 첫 번째 컬럼, TIMESTAMP(3 == precision 얼마나 정밀한지)
...
.column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
...
"""
table = t_env.from_data_stream(
    ds,
    Schema.new_builder()
    .column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
    .column("f1", DataTypes.STRING())
    .column("f2", DataTypes.FLOAT())
    .watermark("ts", "ts - INTERVAL '3' SECOND")
    .build()
).alias("ts", "name", "price")

# table 등록
t_env.create_temporary_table(
    "sink",
    TableDescriptor.for_connector("print")
    .schema(Schema.new_builder()
            .column("name", DataTypes.STRING())
            .column("total_price", DataTypes.FLOAT())
            .column("w_start", DataTypes.TIMESTAMP(3))
            .column("w_end", DataTypes.TIMESTAMP(3))
            .build()
            ).build())

# sliding window 달라지는 부분
t = table.window(
    Slide.over(
        lit(5).seconds).every(lit(2).seconds).on(col("ts")).alias("w")) \
    .group_by(table.name, col("w")) \
    .select(table.name, table.price.sum, col("w").start, col("w").end)

t.execute_insert("sink").wait()

```

## 4-7. SQL 사용 

```python
from pyflink.common.time import Instant
from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import (
    DataTypes, TableDescriptor, Schema, StreamTableEnvironment
)
from pyflink.table.expressions import lit, col
from pyflink.table.window import Tumble

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(
    collection=[
        (Instant.of_epoch_milli(1000), "Alice", 110.1),
        (Instant.of_epoch_milli(4000), "Bob", 30.2),
        (Instant.of_epoch_milli(3000), "Alice", 20.0),
        (Instant.of_epoch_milli(2000), "Bob", 53.1),
        (Instant.of_epoch_milli(5000), "Alice", 13.1),
        (Instant.of_epoch_milli(3000), "Bob", 3.1),
        (Instant.of_epoch_milli(7000), "Bob", 16.1),
        (Instant.of_epoch_milli(10000), "Alice", 20.1),
    ],
    type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))
"""
# timestamp 생성, f0 첫 번째 컬럼, TIMESTAMP(3 == precision 얼마나 정밀한지)
...
.column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
...
"""
table = t_env.from_data_stream(
    ds,
    Schema.new_builder()
    .column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
    .column("f1", DataTypes.STRING())
    .column("f2", DataTypes.FLOAT())
    .watermark("ts", "ts - INTERVAL '3' SECOND")
    .build()
).alias("ts", "name", "price")
# window table 생성
t_env.create_temporary_view('source_table', table)

# Sliding HOP 변경
# 2초 간격으로 뛴다고 해서 HOP
windowed = t_env.sql_query("""
SELECT name, SUM(price) AS total_price,
        HOP_START(ts, INTERVAL '2' SECONDS, INTERVAL '5' SECONDS) as w_start,
        HOP_END(ts, INTERVAL '2' SECONDS, INTERVAL '5' SECONDS) as w_end
FROM source_table
GROUP BY HOP(ts, INTERVAL '2' SECONDS, INTERVAL '5' SECONDS),
name
""")

# table 등록
t_env.create_temporary_table(
    "sink",
    TableDescriptor.for_connector("print")
    .schema(Schema.new_builder()
            .column("name", DataTypes.STRING())
            .column("total_price", DataTypes.FLOAT())
            .column("w_start", DataTypes.TIMESTAMP(3))
            .column("w_end", DataTypes.TIMESTAMP(3))
            .build()
            ).build())

windowed.execute_insert("sink").wait()
```

# 4-8 ~ 4-9. Session Windows 구현

## 4-8. Table API 사용

```python
from pyflink.common.time import Instant
from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import (
    DataTypes, TableDescriptor, Schema, StreamTableEnvironment
)
from pyflink.table.expressions import lit, col
from pyflink.table.window import Session

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(
    collection=[
        (Instant.of_epoch_milli(1000), "Alice", 110.1),
        (Instant.of_epoch_milli(4000), "Bob", 30.2),
        (Instant.of_epoch_milli(3000), "Alice", 20.0),
        (Instant.of_epoch_milli(2000), "Bob", 53.1),
        (Instant.of_epoch_milli(8000), "Bob", 16.1),
        (Instant.of_epoch_milli(10000), "Alice", 20.1),
    ],
    type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))
"""
# timestamp 생성, f0 첫 번째 컬럼, TIMESTAMP(3 == precision 얼마나 정밀한지)
...
.column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
...
"""
table = t_env.from_data_stream(
    ds,
    Schema.new_builder()
    .column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
    .column("f1", DataTypes.STRING())
    .column("f2", DataTypes.FLOAT())
    .watermark("ts", "ts - INTERVAL '3' SECOND")
    .build()
).alias("ts", "name", "price")

# table 등록
t_env.create_temporary_table(
    "sink",
    TableDescriptor.for_connector("print")
    .schema(Schema.new_builder()
            .column("name", DataTypes.STRING())
            .column("total_price", DataTypes.FLOAT())
            .column("w_start", DataTypes.TIMESTAMP(3))
            .column("w_end", DataTypes.TIMESTAMP(3))
            .build()
            ).build())

# 마지막 이벤트로부터 5초 기다릴 수 있다.
t = table.window(Session.with_gap(lit(5).seconds).on(col("ts")).alias("w")) \
    .group_by(table.name, col("w")) \
    .select(table.name, table.price.sum, col("w").start, col("w").end)

t.execute_insert("sink").wait()

""" result
+I[Alice, 130.1, 1970-01-01T09:00:01, 1970-01-01T09:00:08]
+I[Bob, 99.4, 1970-01-01T09:00:02, 1970-01-01T09:00:13]
+I[Alice, 20.1, 1970-01-01T09:00:10, 1970-01-01T09:00:15]
"""
```

## 4-9. SQL  사용

```python
from pyflink.common.time import Instant
from pyflink.common import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import (
    DataTypes, TableDescriptor, Schema, StreamTableEnvironment
)
from pyflink.table.expressions import lit, col
from pyflink.table.window import Tumble

env = StreamExecutionEnvironment.get_execution_environment()
env.set_parallelism(1)
t_env = StreamTableEnvironment.create(env)

ds = env.from_collection(
    collection=[
        (Instant.of_epoch_milli(1000), "Alice", 110.1),
        (Instant.of_epoch_milli(4000), "Bob", 30.2),
        (Instant.of_epoch_milli(3000), "Alice", 20.0),
        (Instant.of_epoch_milli(2000), "Bob", 53.1),
        (Instant.of_epoch_milli(8000), "Bob", 16.1),
        (Instant.of_epoch_milli(10000), "Alice", 20.1),
    ],
    type_info=Types.ROW([Types.INSTANT(), Types.STRING(), Types.FLOAT()]))

table = t_env.from_data_stream(
    ds,
    Schema.new_builder()
    .column_by_expression("ts", "CAST(f0 AS TIMESTAMP(3))")
    .column("f1", DataTypes.STRING())
    .column("f2", DataTypes.FLOAT())
    .watermark("ts", "ts - INTERVAL '3' SECOND")
    .build()
).alias("ts", "name", "price")

t_env.create_temporary_view("source_table", table)
windowed = t_env.sql_query("""
  SELECT 
    name,
    SUM(price) AS total_price,
    SESSION_START(ts, INTERVAL '5' SECONDS) AS w_start,
    SESSION_END(ts, INTERVAL '5' SECONDS) AS w_end
  FROM source_table
  GROUP BY
    SESSION(ts, INTERVAL '5' SECONDS),
    name
""")

t_env.create_temporary_table(
    "sink",
    TableDescriptor.for_connector("print")
    .schema(Schema.new_builder()
            .column("name", DataTypes.STRING())
            .column("total_price", DataTypes.FLOAT())
            .column("w_start", DataTypes.TIMESTAMP(3))
            .column("w_end", DataTypes.TIMESTAMP(3))
            .build()
            ).build())

windowed.execute_insert("sink").wait()
```

# 4-10. State 접근
```python
from pyflink.common import Time
from pyflink.common.typeinfo import Types
from pyflink.datastream import StreamExecutionEnvironment
from pyflink.datastream.functions import KeyedProcessFunction, RuntimeContext
from pyflink.datastream.state import ValueStateDescriptor, StateTtlConfig


class Sum(KeyedProcessFunction):
    """이름을 기준으로 값을 합산하는 객체"""

    def __init__(self):
        self.state = None

    def open(self, runtime_context: RuntimeContext):
        state_descriptor = ValueStateDescriptor("state", Types.FLOAT())
        # ttl state가 얼마나 오랫동안 생존하는지 설정, Time To Leave
        state_ttl_config = StateTtlConfig.new_builder(Time.seconds(1)) \
            .set_update_type(StateTtlConfig.UpdateType.OnReadAndWrite) \
            .disable_cleanup_in_background().build()
        state_descriptor.enable_time_to_live(state_ttl_config)
        self.state = runtime_context.get_state(state_descriptor)

    def process_element(self, value, ctx: 'KeyedProcessFunction.Context'):
        """

        :param value:
        :param ctx: window function 함수를 사용할 때
        :return:
        """
        current = self.state.value()
        if current is None:
            current = 0

        current += value[1]
        self.state.update(current)
        # generator 함수이기 때문에 yield 사용 가능
        # 다음 함수에 값을 넘겨주는 행위
        yield value[0], current


env = StreamExecutionEnvironment.get_execution_environment()

ds = env.from_collection(
    collection=[
        ('Alice', 110.1),
        ('Bob', 30.2),
        ('Alice', 20.1),
        ('Bob', 53.1),
        ('Alice', 13.1),
        ('Bob', 3.1),
        ('Bob', 16.1),
        ('Alice', 20.1),
    ],
    type_info=Types.TUPLE([Types.STRING(), Types.FLOAT()])
)

ds.key_by(lambda x: x[0]).process(Sum()).print()
env.execute()
```

# 4-11. UDF
```python
import json 

from pyflink.table import (
  EnvironmentSettings,
  TableEnvironment,
  DataTypes,
  TableDescriptor,
  Schema
)
from pyflink.table.udf import udf 


t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())

table = t_env.from_elements(
  elements = [
    (1, '{"name": "Spark", "score": 5}'),
    (2, '{"name": "Airflow", "score": 7}'),
    (3, '{"name": "Kafka", "score": 9}'),
    (4, '{"name": "Flink", "score": 8}'),
  ],
  schema = ['id', 'data'])

t_env.create_temporary_table(
  "sink",
  TableDescriptor.for_connector("print")
                 .schema(Schema.new_builder()
                    .column("id", DataTypes.BIGINT())
                    .column("data", DataTypes.STRING())
                    .build())
                 .build())

@udf(result_type=DataTypes.STRING())
def update_score(data):
  json_data = json.loads(data)
  json_data["score"] += 1
  return json.dumps(json_data)

table = table.select(table.id, update_score(table.data))
table.execute_insert("sink").wait()
```